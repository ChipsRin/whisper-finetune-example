import os
import torch
import numpy as np
from datasets import load_dataset
from transformers import (
    WhisperProcessor,
    WhisperForConditionalGeneration,
    TrainingArguments,
    Trainer,
    TrainerCallback,
)
import argparse
from torch.nn.utils.rnn import pad_sequence
import torchaudio
import gc
import psutil
from collections import OrderedDict
import threading
import time
import weakref

# --------- 1. åƒæ•¸è§£æ ---------
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--train_json", type=str, required=True, help="Path to training json file")
    parser.add_argument("--eval_json", type=str, required=True, help="Path to eval json file")
    parser.add_argument("--output_dir", type=str, default="./whisper-finetune-output", help="Output dir for model and logs")
    parser.add_argument("--base_model", type=str, default="openai/whisper-large-v2", help="Base model to finetune")
    parser.add_argument("--learning_rate", type=float, default=1e-5, help="Learning rate")
    parser.add_argument("--batch_size", type=int, default=6, help="Batch size per device")
    parser.add_argument("--num_epochs", type=int, default=3, help="Number of training epochs")
    parser.add_argument("--max_audio_len", type=float, default=30.0, help="Maximum audio length in seconds")
    parser.add_argument("--min_audio_len", type=float, default=0.5, help="Minimum audio length in seconds")
    parser.add_argument("--cache_size_gb", type=float, default=8.0, help="Audio cache size in GB")
    parser.add_argument("--resume_from_checkpoint", type=str, default=None, help="Path to checkpoint to resume from")
    return parser.parse_args()

# --------- 2. è¼•é‡ç´šè¨˜æ†¶é«”å¿«å–ç³»çµ± ---------
class LightweightAudioCache:
    def __init__(self, max_size_gb=2.0):  # å¤§å¹…é™ä½å¿«å–å¤§å°
        self.max_size_bytes = int(max_size_gb * 1024 * 1024 * 1024)
        self.cache = OrderedDict()
        self.current_size = 0
        self.lock = threading.Lock()
        self.hit_count = 0
        self.miss_count = 0

    def _estimate_size(self, audio_array):
        return audio_array.nbytes if hasattr(audio_array, 'nbytes') else len(audio_array) * 4

    def _evict_lru(self, needed_size):
        # æ›´ç©æ¥µçš„æ¸…ç†ç­–ç•¥
        while (self.current_size + needed_size > self.max_size_bytes or
               len(self.cache) > 500) and self.cache:  # é™åˆ¶é …ç›®æ•¸é‡
            oldest_key, oldest_data = self.cache.popitem(last=False)
            self.current_size -= self._estimate_size(oldest_data)
            del oldest_data  # æ˜ç¢ºåˆªé™¤

    def get(self, key):
        with self.lock:
            if key in self.cache:
                self.hit_count += 1
                value = self.cache.pop(key)
                self.cache[key] = value
                return value.copy()  # è¿”å›å‰¯æœ¬é¿å…ä¿®æ”¹
            self.miss_count += 1
            return None

    def put(self, key, value):
        with self.lock:
            if key in self.cache:
                return

            size = self._estimate_size(value)
            # å¦‚æœå–®å€‹æ–‡ä»¶è¶…éå¿«å–å¤§å°çš„10%ï¼Œç›´æ¥è·³é
            if size > self.max_size_bytes * 0.1:
                return

            self._evict_lru(size)
            self.cache[key] = value.copy()
            self.current_size += size

    def clear(self):
        """æ¸…ç©ºå¿«å–"""
        with self.lock:
            self.cache.clear()
            self.current_size = 0
            gc.collect()

    def get_cache_info(self):
        with self.lock:
            hit_rate = self.hit_count / (self.hit_count + self.miss_count) if (self.hit_count + self.miss_count) > 0 else 0
            return {
                'size': len(self.cache),
                'memory_mb': self.current_size / (1024 * 1024),
                'max_memory_mb': self.max_size_bytes / (1024 * 1024),
                'hit_rate': hit_rate
            }

# å…¨å±€å¿«å–å¯¦ä¾‹ - æ¯å€‹é€²ç¨‹ç¨ç«‹
audio_cache = None

def load_audio_fast(audio_path):
    """ä½¿ç”¨torchaudioå¿«é€Ÿè¼‰å…¥éŸ³é »ï¼Œè¨˜æ†¶é«”å„ªåŒ–ç‰ˆ"""
    global audio_cache

    # æª¢æŸ¥å¿«å–
    if audio_cache is not None:
        cached_audio = audio_cache.get(audio_path)
        if cached_audio is not None:
            return cached_audio

    try:
        # ä½¿ç”¨torchaudioè¼‰å…¥ï¼Œæ¯”librosaå¿«å¾ˆå¤š
        waveform, sample_rate = torchaudio.load(audio_path)

        # è½‰æ›ç‚ºå–®è²é“
        if waveform.shape[0] > 1:
            waveform = torch.mean(waveform, dim=0, keepdim=True)

        # é‡æ¡æ¨£åˆ°16kHz (å¦‚æœéœ€è¦) - ä½¿ç”¨å…±äº«çš„resampler
        if sample_rate != 16000:
            # å‰µå»ºå…¨å±€å…±äº«çš„resamplerså­—å…¸
            if not hasattr(load_audio_fast, '_resamplers'):
                load_audio_fast._resamplers = {}

            if sample_rate not in load_audio_fast._resamplers:
                load_audio_fast._resamplers[sample_rate] = torchaudio.transforms.Resample(
                    sample_rate, 16000
                )

            waveform = load_audio_fast._resamplers[sample_rate](waveform)

        # è½‰æ›ç‚ºnumpy
        audio_array = waveform.squeeze().numpy().astype(np.float32)

        # å˜—è©¦å¿«å– (å¦‚æœå•Ÿç”¨ä¸”å¤§å°åˆé©)
        if audio_cache is not None:
            audio_cache.put(audio_path, audio_array)

        # æ˜ç¢ºæ¸…ç†tensor
        del waveform

        return audio_array

    except Exception as e:
        print(f"Error loading audio {audio_path}: {e}")
        return None

# æ·»åŠ è¨˜æ†¶é«”æ¸…ç†å‡½æ•¸
def cleanup_memory():
    """å¼·åˆ¶æ¸…ç†è¨˜æ†¶é«”"""
    gc.collect()
    # ç§»é™¤CUDAæ“ä½œé¿å…å¤šé€²ç¨‹éŒ¯èª¤

def prepare_example_minimal(example):
    """æœ€å°åŒ–é è™•ç†ï¼Œåªä¿ç•™å¿…è¦ä¿¡æ¯"""
    try:
        return {
            "audio_path": example["audio"]["path"],
            "sentence": example["sentence"],
            "duration": example["duration"]
        }
    except Exception as e:
        print(f"Error preparing example: {e}")
        return None

# --------- 3. è¨˜æ†¶é«”å„ªåŒ–çš„è³‡æ–™æ•´ç†å™¨ ---------
class MemoryOptimizedDataCollator:
    def __init__(self, processor):
        self.processor = processor
        self.feature_extractor = processor.feature_extractor
        self.tokenizer = processor.tokenizer
        self.batch_count = 0

    def __call__(self, batch):
        # éæ¿¾Noneæ¨£æœ¬
        batch = [item for item in batch if item is not None]
        if not batch:
            return None

        input_features = []
        labels = []

        try:
            # é€å€‹è™•ç†é¿å…è¨˜æ†¶é«”ç©ç´¯
            for item in batch:
                speech_array = load_audio_fast(item["audio_path"])
                if speech_array is None:
                    continue

                # æå–ç‰¹å¾µ
                features = self.feature_extractor(
                    speech_array,
                    sampling_rate=16000,
                    return_tensors="pt"
                ).input_features[0]

                # è™•ç†æ¨™ç±¤
                label_ids = self.tokenizer(
                    item["sentence"],
                    max_length=448,
                    truncation=True,
                    return_tensors="pt"
                ).input_ids[0]

                input_features.append(features)
                labels.append(label_ids)

                # æ¸…ç†ä¸­é–“è®Šé‡
                del speech_array

            if not input_features:
                return None

            # æ‰¹é‡å¡«å……
            input_features_tensor = torch.stack(input_features)
            labels_tensor = pad_sequence(labels, batch_first=True, padding_value=-100)

            # æ¸…ç†åˆ—è¡¨
            del input_features, labels

            # ç§»é™¤è‡ªå‹•æ¸…ç†é¿å…å¤šé€²ç¨‹CUDAéŒ¯èª¤
            self.batch_count += 1

            return {
                "input_features": input_features_tensor,
                "labels": labels_tensor
            }

        except Exception as e:
            print(f"Error in data collator: {e}")
            # ç™¼ç”ŸéŒ¯èª¤æ™‚åªåšåŸºæœ¬æ¸…ç†
            gc.collect()
            return None

# --------- 4. è¨˜æ†¶é«”ç›£æ§å’Œç®¡ç†å›èª¿ ---------
class MemoryMonitorCallback(TrainerCallback):
    def __init__(self, cache_obj):
        self.cache = cache_obj
        self.train_losses = []
        self.eval_losses = []
        self.memory_warnings = 0

    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs:
            # è¨˜éŒ„loss
            if "loss" in logs:
                self.train_losses.append((logs.get("epoch", 0), logs["loss"]))
            if "eval_loss" in logs:
                self.eval_losses.append((logs.get("epoch", 0), logs["eval_loss"]))

            # æ¯50æ­¥æª¢æŸ¥ä¸€æ¬¡è¨˜æ†¶é«”ç‹€æ…‹
            if state.global_step % 50 == 0:
                memory_info = psutil.virtual_memory()

                # é¡¯ç¤ºè¨˜æ†¶é«”ç‹€æ…‹
                cache_info = "No cache" if self.cache is None else self.cache.get_cache_info()

                if isinstance(cache_info, dict):
                    print(f"Step {state.global_step}: "
                          f"Cache: {cache_info['size']} items "
                          f"({cache_info['memory_mb']:.1f}MB, hit rate: {cache_info['hit_rate']:.2f}), "
                          f"System RAM: {memory_info.percent:.1f}% used")
                else:
                    print(f"Step {state.global_step}: {cache_info}, "
                          f"System RAM: {memory_info.percent:.1f}% used")

                # è¨˜æ†¶é«”è­¦å‘Šå’Œè™•ç†
                if memory_info.percent > 90:
                    self.memory_warnings += 1
                    print(f"âš ï¸  HIGH MEMORY WARNING {self.memory_warnings}: {memory_info.percent:.1f}% RAM used")

                    # æ¸…ç†å¿«å–
                    if self.cache is not None:
                        self.cache.clear()
                        print("ğŸ—‘ï¸  Cache cleared to free memory")

                    # å¼·åˆ¶åƒåœ¾å›æ”¶ (é¿å…CUDAæ“ä½œ)
                    gc.collect()
                    print("ğŸ§¹ Memory cleanup completed")

                    # è¨˜æ†¶é«”å±éšªæ™‚æš«åœè¨“ç·´
                    if memory_info.percent > 95:
                        print("ğŸš¨ CRITICAL MEMORY USAGE! Pausing training...")
                        control.should_save = True
                        # ä¸ç›´æ¥åœæ­¢ï¼Œè®“ä½¿ç”¨è€…æ±ºå®š

    def on_train_end(self, args, state, control, **kwargs):
        import matplotlib.pyplot as plt

        # ç¹ªè£½lossæ›²ç·š
        if self.train_losses or self.eval_losses:
            plt.figure(figsize=(10, 6))

            if self.train_losses:
                epochs, values = zip(*self.train_losses)
                plt.plot(epochs, values, label="Train Loss", alpha=0.7)

            if self.eval_losses:
                epochs, values = zip(*self.eval_losses)
                plt.plot(epochs, values, label="Eval Loss", linewidth=2)

            plt.xlabel("Epoch")
            plt.ylabel("Loss")
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.title("Training Progress - Large Dataset Optimized")
            plt.savefig(os.path.join(args.output_dir, "loss_curve_optimized.png"), dpi=150)
            plt.close()

# --------- 5. ä¸»ç¨‹å¼ ---------
if __name__ == "__main__":
    args = parse_args()

    # åˆå§‹åŒ–è¼•é‡ç´šå¿«å– (å¤§å¹…é™ä½å¿«å–å¤§å°)
    cache_size = min(args.cache_size_gb, 50.0)  # æœ€å¤š50GB
    audio_cache = LightweightAudioCache(max_size_gb=cache_size)

    print(f"Loading datasets with smart caching ({args.cache_size_gb}GB)...")

    # è¼‰å…¥è³‡æ–™é›†
    train_dataset = load_dataset("json", data_files=args.train_json, split="train")
    eval_dataset = load_dataset("json", data_files=args.eval_json, split="train")

    print("Loading processor and model...")
    processor = WhisperProcessor.from_pretrained(args.base_model, language="Chinese", task="transcribe")
    model = WhisperForConditionalGeneration.from_pretrained(args.base_model)

    print("Filtering datasets by duration...")
    train_dataset = train_dataset.filter(
        lambda x: args.min_audio_len <= x["duration"] <= args.max_audio_len,
        desc="Filtering train data"
    )
    eval_dataset = eval_dataset.filter(
        lambda x: args.min_audio_len <= x["duration"] <= args.max_audio_len,
        desc="Filtering eval data"
    )

    print(f"Dataset sizes - Train: {len(train_dataset)}, Eval: {len(eval_dataset)}")

    print("Preparing datasets (minimal preprocessing)...")
    train_dataset = train_dataset.map(
        prepare_example_minimal,
        num_proc=16,  # å¢åŠ ä¸¦è¡Œè™•ç†
        desc="Preparing train data"
    )

    eval_dataset = eval_dataset.map(
        prepare_example_minimal,
        num_proc=16,
        desc="Preparing eval data"
    )

    # æ¸…ç†è¨˜æ†¶é«”
    gc.collect()

    # é«˜æ•ˆèƒ½è¨“ç·´åƒæ•¸
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.batch_size,
        gradient_accumulation_steps=4,
        num_train_epochs=args.num_epochs,
        learning_rate=args.learning_rate,
        warmup_steps=500,
        logging_steps=50,  # æ›´é »ç¹çš„logging
        save_steps=1000,
        save_total_limit=3,
        # --- ä¿®æ”¹: è¼‰å…¥evalæœ€ä½³æ¨¡å‹ ---
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        # Traineræœƒè‡ªå‹•ä¿è­·best checkpointä¸è¢«save_total_limitåˆªé™¤
        # æ‰€ä»¥3å€‹checkpoint = best + æœ€è¿‘2å€‹
        # ---------------------------------
        logging_dir=os.path.join(args.output_dir, "logs"),
        evaluation_strategy="steps",
        eval_steps=1000,
        bf16=True,
        remove_unused_columns=False,
        # é™ä½workersé¿å…CUDAå¤šé€²ç¨‹å•é¡Œ
        dataloader_num_workers=4,  # é™ä½é¿å…CUDAéŒ¯èª¤
        dataloader_pin_memory=True,
        dataloader_persistent_workers=True,
        dataloader_prefetch_factor=2,  # é™ä½é å–
        report_to="tensorboard",
        ddp_find_unused_parameters=False,
        optim="adamw_torch_fused",
        max_grad_norm=1.0,
        dataloader_drop_last=True,
        ignore_data_skip=True,
        # è¨˜æ†¶é«”å„ªåŒ–
        max_steps=-1,
        lr_scheduler_type="cosine",
        weight_decay=0.01,
    )

    # å»ºç«‹è¨˜æ†¶é«”å„ªåŒ–è¨“ç·´å™¨
    data_collator = MemoryOptimizedDataCollator(processor)
    memory_callback = MemoryMonitorCallback(audio_cache)

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
        tokenizer=processor.tokenizer,
        callbacks=[memory_callback]
    )

    print("Starting optimized training for large dataset...")
    print(f"Cache configuration: {args.cache_size_gb}GB, {training_args.dataloader_num_workers} workers")
    print(f"Best model selection: enabled (metric=eval_loss, lower is better)")

    # é–‹å§‹è¨“ç·´ (æ”¯æ´å¾checkpointæ¢å¾©)
    trainer.train(resume_from_checkpoint=args.resume_from_checkpoint)

    # è¨“ç·´çµæŸå¾Œ model å·²ç¶“æ˜¯ eval_loss æœ€ä½çš„é‚£å€‹ (ç”± load_best_model_at_end=True ä¿è­‰)
    print("Saving best model (lowest eval_loss)...")
    trainer.save_model(os.path.join(training_args.output_dir, "best_model"))
    processor.save_pretrained(training_args.output_dir)

    # é¡¯ç¤ºæœ€çµ‚å¿«å–çµ±è¨ˆ
    final_cache_info = audio_cache.get_cache_info()
    print(f"Training completed!")
    print(f"Best model saved to: {os.path.join(training_args.output_dir, 'best_model')}")
    print(f"Final cache stats: {final_cache_info['size']} items, "
          f"{final_cache_info['memory_mb']:.1f}MB used")
